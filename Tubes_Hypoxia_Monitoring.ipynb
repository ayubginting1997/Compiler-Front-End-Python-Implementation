{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tubes_Hypoxia Monitoring.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SEAOdDCWchAy",
        "vxhgsECRSqX-",
        "-KfKGrJT7hJR",
        "YhvVSQ5o_sEo",
        "SkiVsFAM1LxR",
        "qu02aIYKyfLZ",
        "5_DJ5505y1uK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayubginting1997/Compiler-Front-End-Python-Implementation/blob/master/Tubes_Hypoxia_Monitoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5gTQNiD576w"
      },
      "source": [
        "# PPG memanfaatkan Flash dan Camera Handphone\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_XWoR9xl88X"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from scipy import signal\n",
        "import os\n",
        "import shutil\n",
        "\"\"\"\n",
        "# This Function Works as follows:\n",
        "1. Takes input as the path of the finger video \n",
        "2. Returns the resting heartrate in bpm as a number \n",
        "\n",
        "\n",
        "- It first gets frames from video at rate 20 frames per second\n",
        "- Extract channels from the images \n",
        "- then it computes the average of the red channels and saves them in pixels_averages\n",
        "- Then do a HPF to the average\n",
        "- Truncate wonky data from  beginning of series\n",
        "- Take FFT to the filtered data and the compute the heartrate in bpm.\n",
        "\"\"\"\n",
        "overall_results = []\n",
        "\n",
        "def HeartRateFinger(Video_Path):\n",
        "    os.makedirs(\"frames\")  #make a directory called frames to save the images in\n",
        "    vidcap = cv2.VideoCapture(Video_Path)  #the directory path to the video of interest\n",
        "    images = []\n",
        "\n",
        "    #take in images and get indidvual frames\n",
        "    def getFrame(sec):\n",
        "        vidcap.set(cv2.CAP_PROP_POS_MSEC, sec * 1000)  #VideoCaptureProperties to capture \n",
        "        hasFrames, image = vidcap.read()\n",
        "        if hasFrames:\n",
        "            cv2.imwrite(\"frames/image\" + str(count) + \".jpg\", image)  # save frame as JPG file\n",
        "        images.append(\"frames/image\" + str(count) + \".jpg\")\n",
        "        return hasFrames\n",
        "\n",
        "    #set up frame rate\n",
        "    sec = 0\n",
        "    frameRate = 0.05  #capture frame each 0.05 seconds --> (1 second) / .05 = 20 seconds 20 fps\n",
        "    count = 1\n",
        "    success = getFrame(sec)  #success tracker\n",
        "    while (success):\n",
        "        count = count + 1\n",
        "        sec = sec + frameRate\n",
        "        sec = round(sec, 2)\n",
        "        success = getFrame(sec)\n",
        "\n",
        "    #Get the channels from images and Find the average of RED channel for each frame\n",
        "    pixels_averages = []\n",
        "    for i in range(len(images) - 1):\n",
        "        bgr_image = cv2.imread(images[i])\n",
        "        blue_channel, green_channel, red_channel = cv2.split(bgr_image)  #splits into the 3 color channels (RGB)\n",
        "        average = np.mean(red_channel)  #mean of all the red channels  is the sum all pixels in channel and divide by number of pixels\n",
        "        pixels_averages.append(average)  #add this average to all the other frames array\n",
        "    pixels_averages = np.divide(np.array(pixels_averages), 255)  #normalize our averages between 0-1\n",
        "\n",
        "    #Clean and filter data (butterworth filter)\n",
        "    sos = signal.butter(4, 1, 'hp', fs=20, output='sos')  # setting up signal.butter(kind of frequency,framerate, name of method)\n",
        "    filtered = signal.sosfilt(sos, pixels_averages) #Applying filter\n",
        "\n",
        "    # truncate wonky data from  beginning of series\n",
        "    filtered = filtered[40:]\n",
        "\n",
        "    #Perform FFT to find frequency of max amplitude\n",
        "    Sample_rate = 20  #same as the frame rate is the sampling rate\n",
        "    BW = Sample_rate / 2  #bandwidth (range of frequency) in signal processing\n",
        "    fft = np.absolute(np.fft.fft(filtered))\n",
        "    frames_len = len(filtered)\n",
        "    frequancies = np.arange(0, BW, Sample_rate / frames_len)\n",
        "    fft = fft[0:len(frequancies)]\n",
        "\n",
        "    #Convert back to BPM from hz\n",
        "    heartrate = np.round(frequancies[np.argmax(fft)] * 60)  #take the highest peak frequency and multiply by 60 and round it\n",
        "    shutil.rmtree(\"frames\")  #remove folder made to keep the directory clean\n",
        "    overall_results.append(heartrate)\n",
        "    return heartrate\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saNBuC32l88Z",
        "outputId": "2b91ecf8-5660-4458-c862-8dcb4e36b346"
      },
      "source": [
        "#script expected outcome while Resting is 74-78 BPM\n",
        "path = \"/content/sample_data/95.MOV\"\n",
        "my_heart_rate = HeartRateFinger(path)\n",
        "print(\"Your Heart rate is : {} bpm\".format(my_heart_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your Heart rate is : 77.0 bpm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbM0ohdGl88Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6ca019-93fc-4711-9e35-9bffac5b1d4e"
      },
      "source": [
        "#script expected outcome while Resting is 69-77 BPM\n",
        "path = \"/content/sample_data/resting1.mp4\"\n",
        "my_heart_rate = HeartRateFinger(path)\n",
        "print(\"Your Heart rate is : {} bpm\".format(my_heart_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your Heart rate is : 70.0 bpm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYVR4sPNl88a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b128ba2-729f-49ba-8a88-badf67935aea"
      },
      "source": [
        "#script expected outcome while Resting is 69-77 BPM\n",
        "path = \"/content/sample_data/RESTING2.mp4\"\n",
        "my_heart_rate = HeartRateFinger(path)\n",
        "print(\"Your Heart rate is : {} bpm\".format(my_heart_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your Heart rate is : 70.0 bpm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVJRXYPhl88a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b7472e-9421-40ab-a1e7-070a89785209"
      },
      "source": [
        "#script expected outcome after activity was 83-88 BPM (50 jumping jacks)\n",
        "path = \"/content/sample_data/active1.mp4\"\n",
        "my_heart_rate = HeartRateFinger(path)\n",
        "print(\"Your Heart rate is : {} bpm\".format(my_heart_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your Heart rate is : 85.0 bpm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6CzpTOsl88a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5622f3f-4fe1-4ea9-daac-81b867c71d18"
      },
      "source": [
        "#script expected outcome after activity was 75-85 BPM (jogging for a bit)\n",
        "path =  \"/content/sample_data/active2.mp4\"\n",
        "my_heart_rate = HeartRateFinger(path)\n",
        "print(\"Your Heart rate is : {} bpm\".format(my_heart_rate))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your Heart rate is : 75.0 bpm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5vWNzahl88b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b202e0-2127-4a2b-cab2-7bbe6cd2d5a8"
      },
      "source": [
        "#Overall results laid out with their error rate as well\n",
        "print(overall_results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[77.0, 85.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEAOdDCWchAy"
      },
      "source": [
        "# PPG PREDICTION "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "o_jbV1AlSRYT",
        "outputId": "c666872f-6a3f-4cef-ab1a-f019a639dfbc"
      },
      "source": [
        "import pandas as pd\n",
        "from calcul_features import calcul_features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# chargement des labels\n",
        "SBP_label = pd.read_csv(\"/media/PPGonMObile/DATASET/SBP_label.csv\", delimiter = ';', header = None)\n",
        "DBP_label = pd.read_csv(\"/media/PPGonMObile/DATASET/DBP_label.csv\", delimiter = ';', header = None)\n",
        "\n",
        "# chargement des fichiers segmentés\n",
        "fichier_segmente = pd.read_csv(\"/media/PPGonMObile/DATASET/segmente.csv\", delimiter = ';', header = None)\n",
        "# chargement des features liés à l'anthropometrie du sujet : age, poids, taille, sex\n",
        "features_anthropo = pd.read_csv(\"/media/PPGonMObile/DATASET/features_anthropo.csv\", delimiter = ';')\n",
        "\n",
        "BUFFER_SIZE = 1200\n",
        "\n",
        "# on calcule tous les features liés au signal et on ajoute aussi les features anthropométriques au même tableau de features\n",
        "features = calcul_features(fichier_segmente,features_anthropo,SBP_label,BUFFER_SIZE)\n",
        "\n",
        "# permet de choisir quels features on souhaite garder pour l'entrainement du modèles\n",
        "features = pd.concat([features.PPG_mean, features.PPG_min,features.PPG_max,features.PPG_amp,features.PPG_q_0_75, \n",
        "                                    features.PPG_q_0_25,features.PPG_0_cross,features.PPG_kurt,features.PPG_var,features.PPG_len, \n",
        "                                    features.PPG_S_len, features.PPG_D_len,features.PPG_int, features.PPG_S_int, features.PPG_D_int,\n",
        "                                    features.PPG_len_S_tot,features.PPG_len_D_tot, features.PPG_len_D_S, \n",
        "                                    features.PPG_int_S_tot, features.PPG_int_D_tot, features.PPG_int_D_S,features.PPG_max_D, \n",
        "                                    features.PPG_S_High_0_10, features.PPG_S_High_0_25,features.PPG_S_High_0_33,features.PPG_S_High_0_50,\n",
        "                                    features.PPG_S_High_0_66,features.PPG_S_High_0_75,\n",
        "                                    features.PPG_D_High_0_10, features.PPG_D_High_0_25,features.PPG_D_High_0_33,features.PPG_D_High_0_50,\n",
        "                                    features.PPG_D_High_0_66,features.PPG_D_High_0_75,\n",
        "                                    features.sex, features.age, features.weight, features.height, features.bmi], axis =1)\n",
        "\n",
        "X_train = features.values\n",
        "\n",
        "# le signal est dejà normalisé mais il est important de normaliser/scaler les features entre eux\n",
        "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
        "#min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_train = max_abs_scaler.fit_transform(X_train)\n",
        "#X_train = min_max_scaler.fit_transform(X_train)\n",
        "#X_train = normalize(X_train, norm = 'max')\n",
        "\"\"\"sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\"\"\"\n",
        "\n",
        "# on divise les données en données d'entrainement et de test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, SBP_label, test_size=0.2, random_state=0)\n",
        "\n",
        "# utiliser de gridsearchCV pour extraire les paramètres les plus appropriés\n",
        "# intéressant aussi car cross validation auto\n",
        "\n",
        "#random forest regressor\n",
        "params_rfr = {\n",
        "    \"n_estimators\": [5,10,15,20,25,30,35],\n",
        "    \"max_depth\" : [10,15,20,25,30],\n",
        "    \"random_state\" : [0]\n",
        "}\n",
        "rfr = RandomForestRegressor()\n",
        "gsc_rfr = GridSearchCV(rfr, params_rfr, cv=7)\n",
        "\n",
        "gsc_rfr.fit(X_train, np.ravel(y_train))\n",
        "y_pred = gsc_rfr.best_estimator_.predict(X_test)\n",
        "\n",
        "# decision tree regressor\n",
        "\"\"\"dtr = DecisionTreeRegressor()\n",
        "params_dtr = {\n",
        "    \"max_depth\" : [10,15,20,25,30],\n",
        "}\n",
        "gsc_dtr = GridSearchCV(dtr, params_dtr, cv=7)\n",
        "gsc_dtr.fit(X_train, np.ravel(y_train))\n",
        "y_pred = gsc_dtr.best_estimator_.predict(X_test)\"\"\"\n",
        "\n",
        "# support vector regressor\n",
        "\"\"\"params_svr = {\n",
        "    \"kernel\": ['linear'],\n",
        "    \"C\" : [100],\n",
        "    \"gamma\" : ['auto']\n",
        "}\n",
        "svr = SVR()\n",
        "gsc_svr = GridSearchCV(svr, params_svr, cv=7)\n",
        "gsc_svr.fit(X_train, np.ravel(y_train))\n",
        "y_pred = gsc_svr.best_estimator_.predict(X_test)\"\"\"\n",
        "\n",
        "# adaboost regressor\n",
        "'''adb = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
        "adb.fit(X_train, y_train)\n",
        "y_pred = adb.predict(X_test)'''\n",
        "\n",
        "\"\"\"adb = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
        "adb.fit(X_train, y_train)\n",
        "y_pred = adb.predict(X_test)\"\"\"\n",
        "\n",
        "# linear regression\n",
        "\"\"\"lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_test)\"\"\"\n",
        "\n",
        "# decision tree regression\n",
        "\"\"\"dtr = DecisionTreeRegressor()\n",
        "dtr.fit(X_train, y_train)\n",
        "y_pred = dtr.predict(X_test)\"\"\"\n",
        "\n",
        "y_pred = y_pred.reshape((-1,1))\n",
        "y_test = y_test.values\n",
        "\n",
        "error_moy = np.mean(y_pred-y_test)\n",
        "std = np.std(y_pred-y_test)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(y_test, y_pred, 'bo')\n",
        "plt.title('SBP predite en fonction de SBP réelle')\n",
        "plt.xlabel('SBP réelle')\n",
        "plt.ylabel('SBP prédite')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print('Erreur moyenne',error_moy)\n",
        "print('Ecart type',std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-745f0474918f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcalcul_features\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcalcul_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxAbsScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'calcul_features'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxhgsECRSqX-"
      },
      "source": [
        "# CALCULATION PPG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vih9LVySoTm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import kurtosis\n",
        "from scipy.stats import skew\n",
        "from scipy import integrate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calcul_features(PPG,features_anthropo, Ytrain,BUFFER_SIZE):\n",
        "    #calcul des features pour la prediction\n",
        "    PPG = PPG.values\n",
        "    feature = pd.DataFrame(index = Ytrain, columns = ['PPG_mean', 'PPG_min','PPG_max','PPG_std','PPG_amp','PPG_q_0_75', \n",
        "                                        'PPG_q_0_25','PPG_0_cross','PPG_kurt','PPG_skew','PPG_var','PPG_len', 'PPG_S_len', 'PPG_D_len',\n",
        "                                        'PPG_int', 'PPG_S_int', 'PPG_D_int','PPG_len_S_tot','PPG_len_D_tot', 'PPG_len_D_S', \n",
        "                                        'PPG_int_S_tot', 'PPG_int_D_tot', 'PPG_int_D_S','PPG_max_D', \n",
        "                                        'PPG_S_High_0_10', 'PPG_S_High_0_25','PPG_S_High_0_33','PPG_S_High_0_50','PPG_S_High_0_66','PPG_S_High_0_75',\n",
        "                                        'PPG_D_High_0_10', 'PPG_D_High_0_25','PPG_D_High_0_33','PPG_D_High_0_50','PPG_D_High_0_66','PPG_D_High_0_75',\n",
        "                                        'sex', 'age', 'weight', 'height', 'bmi',\n",
        "                                        'D_PPG_mean','D_PPG_min','D_PPG_max','D_PPG_std','D_PPG_amp','D_PPG_q_0_75', \n",
        "                                        'D_PPG_q_0_25','D_PPG_0_cross','D_PPG_kurt','D_PPG_skew'])\n",
        "    \n",
        "    PPG_mean = np.zeros(PPG.shape[0]) # moyenne signal\n",
        "    PPG_min = np.zeros(PPG.shape[0]) # minimum signal\n",
        "    PPG_max = np.zeros(PPG.shape[0]) # max signal\n",
        "    PPG_std = np.zeros(PPG.shape[0]) # ecart type signal\n",
        "    PPG_amp = np.zeros(PPG.shape[0]) # amplitude signal\n",
        "    PPG_0_75 = np.zeros(PPG.shape[0]) # percentile 0.75 signal\n",
        "    PPG_0_25 = np.zeros(PPG.shape[0]) # percentile 0.25 signal\n",
        "    PPG_0_cross = np.zeros(PPG.shape[0]) # nombre d'intersection à 0\n",
        "    PPG_kurt = np.zeros(PPG.shape[0]) # kurtosis signal\n",
        "    PPG_skew = np.zeros(PPG.shape[0]) # skewness signal\n",
        "    PPG_var = np.zeros(PPG.shape[0]) # variance signal\n",
        "    PPG_len = np.zeros(PPG.shape[0]) # longueur du signal\n",
        "    PPG_S_len = np.zeros(PPG.shape[0]) # longueur temps systolique\n",
        "    PPG_D_len = np.zeros(PPG.shape[0]) # lo,gueur temps diastolique\n",
        "    PPG_int = np.zeros(PPG.shape[0]) # integrale signal\n",
        "    PPG_S_int = np.zeros(PPG.shape[0]) # integrale aire systolique\n",
        "    PPG_D_int = np.zeros(PPG.shape[0]) # integrale aire diastolique\n",
        "    PPG_len_S_tot = np.zeros(PPG.shape[0]) # rapppport temps systolique/temps total\n",
        "    PPG_len_D_tot = np.zeros(PPG.shape[0]) # rapppport temps diastolique/temps total\n",
        "    PPG_len_D_S = np.zeros(PPG.shape[0]) # rapppport temps diastolique/temps systolique\n",
        "    PPG_int_S_tot = np.zeros(PPG.shape[0]) # rapppport integrale systolique/integrale total\n",
        "    PPG_int_D_tot = np.zeros(PPG.shape[0]) # rapppport integrale diastolique/integrale total\n",
        "    PPG_int_D_S = np.zeros(PPG.shape[0]) # rapppport integrale diastolique/integrale systolique\n",
        "    PPG_max_D = np.zeros(PPG.shape[0]) # max de la dérivéé du signal\n",
        "    PPG_S_High_0_10 = np.zeros(PPG.shape[0]) # abscisse à 0.10*max_abs côté gauche\n",
        "    PPG_S_High_0_25 = np.zeros(PPG.shape[0]) # abscisse à 0.25*max_abs côté gauche\n",
        "    PPG_S_High_0_33 = np.zeros(PPG.shape[0]) # abscisse à 0.33*max_abs côté gauche\n",
        "    PPG_S_High_0_50 = np.zeros(PPG.shape[0]) # abscisse à 0.50*max_abs côté gauche\n",
        "    PPG_S_High_0_66 = np.zeros(PPG.shape[0]) # abscisse à 0.66*max_abs côté gauche\n",
        "    PPG_S_High_0_75 = np.zeros(PPG.shape[0]) # abscisse à 0.75*max_abs côté gauche\n",
        "    \n",
        "    PPG_D_High_0_10 = np.zeros(PPG.shape[0]) # abscisse à 0.10*max_abs côté droit\n",
        "    PPG_D_High_0_25 = np.zeros(PPG.shape[0]) # abscisse à 0.25*max_abs côté droit\n",
        "    PPG_D_High_0_33 = np.zeros(PPG.shape[0]) # abscisse à 0.33*max_abs côté droit\n",
        "    PPG_D_High_0_50 = np.zeros(PPG.shape[0]) # abscisse à 0.50*max_abs côté droit\n",
        "    PPG_D_High_0_66 = np.zeros(PPG.shape[0]) # abscisse à 0.66*max_abs côté droit\n",
        "    PPG_D_High_0_75 = np.zeros(PPG.shape[0]) # abscisse à 0.75*max_abs côté droit\n",
        "    \n",
        "    D_PPG = np.zeros((PPG.shape[0],PPG.shape[1])) # creation du vecteur derivee du signal\n",
        "    D_PPG_mean = np.zeros(PPG.shape[0]) # moyenne derivee signal\n",
        "    D_PPG_min = np.zeros(PPG.shape[0]) # min derivee signal\n",
        "    D_PPG_max = np.zeros(PPG.shape[0]) # max derivee signal \n",
        "    D_PPG_std = np.zeros(PPG.shape[0]) # ecart type derivee signal\n",
        "    D_PPG_amp = np.zeros(PPG.shape[0]) # amplitude derivee\n",
        "    D_PPG_0_75 = np.zeros(PPG.shape[0]) # percentile 0.75 derivee\n",
        "    D_PPG_0_25 = np.zeros(PPG.shape[0]) # percentile 0.25 derivee\n",
        "    D_PPG_0_cross = np.zeros(PPG.shape[0]) # intersections à 0 de la derivee\n",
        "    D_PPG_kurt = np.zeros(PPG.shape[0]) # kurtosis de la derivee\n",
        "    D_PPG_skew = np.zeros(PPG.shape[0]) # skewness de la derivee\n",
        "    \n",
        "    for i in range(PPG.shape[0]):\n",
        "        cpt = 1\n",
        "        # Buffer_size doit être égal au size_exp du fichier read.py\n",
        "        # la fin du signal est composée de 0, on revient en arrière jusqu'à ce qu'il n'y est plus de 0 pour\n",
        "        # connaitre la taille exacte du signal\n",
        "        # une fois celle-ci connue on peut calculer les features sur le signal\n",
        "        while PPG[i,BUFFER_SIZE - cpt] == PPG[i,BUFFER_SIZE-1]:\n",
        "            cpt = cpt + 1\n",
        "            \n",
        "        \"\"\"if i < 10:\n",
        "            plt.figure()\n",
        "            plt.plot(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "            plt.show()\"\"\"\n",
        "            \n",
        "        PPG_mean[i] =  np.mean(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_min[i] =  np.min(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_max[i] =  np.max(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_std[i] =  np.std(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_amp[i] =  np.max(PPG[i,0:BUFFER_SIZE - cpt]) - np.min(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_0_75[i] = np.quantile(PPG[i,0:BUFFER_SIZE - cpt], 0.75)\n",
        "        PPG_0_25[i] = np.quantile(PPG[i,0:BUFFER_SIZE - cpt], 0.25)\n",
        "        PPG_0_cross[i] = len(np.where(np.diff(np.sign(PPG[i,0:BUFFER_SIZE - cpt])))[0])\n",
        "        PPG_kurt[i] = kurtosis(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_skew[i] =  skew(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        \n",
        "        if np.mean(PPG[i,0:BUFFER_SIZE - cpt]) == 0:\n",
        "            PPG_var[i] =  float(np.std(PPG[i,0:BUFFER_SIZE - cpt]))\n",
        "        else:\n",
        "            PPG_var[i] =  float(np.std(PPG[i,0:BUFFER_SIZE - cpt])) / float(np.mean(PPG[i,0:BUFFER_SIZE - cpt]))\n",
        "        \n",
        "        PPG_len[i] = BUFFER_SIZE-cpt\n",
        "        PPG_S_len[i] = np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_D_len[i] = BUFFER_SIZE - cpt - np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        PPG_int[i] = np.trapz(PPG[i,0:BUFFER_SIZE - cpt], dx = 1/1000)\n",
        "        PPG_S_int[i] = np.trapz(PPG[i,0:np.argmax(PPG[i,0:BUFFER_SIZE - cpt])], dx = 1/1000)\n",
        "        PPG_D_int[i] = np.trapz(PPG[i,np.argmax(PPG[i,0:BUFFER_SIZE - cpt]):BUFFER_SIZE - cpt], dx = 1/1000)\n",
        "        PPG_len_S_tot[i] = PPG_S_len[i]/PPG_len[i]\n",
        "        PPG_len_D_tot[i] = PPG_D_len[i]/PPG_len[i]\n",
        "        PPG_len_D_S[i] = PPG_D_len[i]/PPG_S_len[i]\n",
        "        PPG_int_S_tot[i] = PPG_S_int[i]/PPG_int[i]\n",
        "        PPG_int_D_tot[i] = PPG_D_int[i]/PPG_int[i]\n",
        "        PPG_int_D_S[i] = PPG_D_int[i]/PPG_S_int[i]\n",
        "        PPG_max_D[i] = np.max(np.gradient(PPG[i,0:BUFFER_SIZE - cpt], 1/1000))\n",
        "        \n",
        "        for k in range(np.argmax(PPG[i,0:BUFFER_SIZE - cpt])):\n",
        "            if PPG[i,k] < 0.10 * PPG_max[i] and PPG[i,k+1] > 0.10 * PPG_max[i]:\n",
        "                PPG_S_High_0_10[i] = np.argmax(PPG[i,0:BUFFER_SIZE - cpt]) - k\n",
        "                \n",
        "            if PPG[i,k] < 0.25 * PPG_max[i] and PPG[i,k+1] > 0.25 * PPG_max[i]:\n",
        "                PPG_S_High_0_25[i] = np.argmax(PPG[i,0:BUFFER_SIZE - cpt]) - k\n",
        "            \n",
        "            if PPG[i,k] < 0.33 * PPG_max[i] and PPG[i,k+1] > 0.33 * PPG_max[i]:\n",
        "                PPG_S_High_0_33[i] = np.argmax(PPG[i,0:BUFFER_SIZE - cpt]) - k\n",
        "            \n",
        "            if PPG[i,k] < 0.50 *PPG_max[i] and PPG[i,k+1] > 0.50 * PPG_max[i]:\n",
        "                PPG_S_High_0_50[i] = np.argmax(PPG[i,0:BUFFER_SIZE - cpt]) - k\n",
        "                \n",
        "            if PPG[i,k] < 0.66 *PPG_max[i] and PPG[i,k+1] > 0.66 * PPG_max[i]:\n",
        "                PPG_S_High_0_66[i] = np.argmax(PPG[i,0:BUFFER_SIZE - cpt]) - k\n",
        "            \n",
        "            if PPG[i,k] < 0.75 * PPG_max[i] and PPG[i,k+1] > 0.75 * PPG_max[i]:\n",
        "                PPG_S_High_0_75[i] = np.argmax(PPG[i,0:BUFFER_SIZE - cpt]) - k\n",
        "        \n",
        "        \n",
        "        for k in range(np.argmax(PPG[i,0:BUFFER_SIZE - cpt]),BUFFER_SIZE - cpt):\n",
        "            if PPG[i,k] > 0.10 * PPG_max[i]and PPG[i,k+1] < 0.10 * PPG_max[i]:\n",
        "                PPG_D_High_0_10[i] = k - np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "                \n",
        "            if PPG[i,k] > 0.25 * PPG_max[i] and PPG[i,k+1] < 0.25 * PPG_max[i]:\n",
        "                PPG_D_High_0_25[i] = k - np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "            \n",
        "            if PPG[i,k] > 0.33 * PPG_max[i] and PPG[i,k+1] < 0.33 * PPG_max[i]:\n",
        "                PPG_D_High_0_33[i] = k - np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "            \n",
        "            if PPG[i,k] > 0.50 * PPG_max[i] and PPG[i,k+1] < 0.50 * PPG_max[i]:\n",
        "                PPG_D_High_0_50[i] = k - np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "                \n",
        "            if PPG[i,k] > 0.66 * PPG_max[i] and PPG[i,k+1] < 0.66 * PPG_max[i]:\n",
        "                PPG_D_High_0_66[i] = k - np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "            \n",
        "            if PPG[i,k] > 0.75 * PPG_max[i] and PPG[i,k+1] < 0.75 * PPG_max[i]:\n",
        "                PPG_D_High_0_75[i] = k - np.argmax(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "                \n",
        "        D_PPG[i,:] = np.gradient(PPG[i,:], 1/1000)\n",
        "        \"\"\"print(D_PPG)\n",
        "        plt.figure()\n",
        "        plt.plot(D_PPG)\n",
        "        plt.show()\"\"\"\n",
        "        \n",
        "        D_PPG_mean[i] =  np.mean(D_PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        D_PPG_min[i] =  np.min(D_PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        D_PPG_max[i] =  np.max(D_PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        D_PPG_std[i] =  np.std(D_PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        D_PPG_amp[i] =  np.max(D_PPG[i,0:BUFFER_SIZE - cpt]) - np.min(PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        D_PPG_0_75[i] = np.quantile(D_PPG[i,0:BUFFER_SIZE - cpt], 0.75)\n",
        "        D_PPG_0_25[i] = np.quantile(D_PPG[i,0:BUFFER_SIZE - cpt], 0.25)\n",
        "        D_PPG_0_cross[i] = len(np.where(np.diff(np.sign(D_PPG[i,0:BUFFER_SIZE - cpt])))[0])\n",
        "        D_PPG_kurt[i] = kurtosis(D_PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        D_PPG_skew[i] =  skew(D_PPG[i,0:BUFFER_SIZE - cpt])\n",
        "        \n",
        "    feature.PPG_mean = PPG_mean\n",
        "    feature.PPG_min = PPG_min\n",
        "    feature.PPG_max = PPG_max\n",
        "    feature.PPG_std = PPG_std\n",
        "    feature.PPG_amp = PPG_amp\n",
        "    feature.PPG_q_0_75 = PPG_0_75\n",
        "    feature.PPG_q_0_25 = PPG_0_25\n",
        "    feature.PPG_0_cross = PPG_0_cross\n",
        "    feature.PPG_kurt = PPG_kurt\n",
        "    feature.PPG_skew = PPG_skew\n",
        "    feature.PPG_var = PPG_var\n",
        "    feature.PPG_len = PPG_len\n",
        "    feature.PPG_S_len = PPG_S_len\n",
        "    feature.PPG_D_len = PPG_D_len\n",
        "    feature.PPG_int = PPG_int\n",
        "    feature.PPG_S_int = PPG_S_int\n",
        "    feature.PPG_D_int = PPG_D_int\n",
        "    feature.PPG_len_S_tot = PPG_len_S_tot\n",
        "    feature.PPG_len_D_tot = PPG_len_D_tot\n",
        "    feature.PPG_len_D_S = PPG_len_D_S\n",
        "    feature.PPG_int_S_tot = PPG_int_S_tot\n",
        "    feature.PPG_int_D_tot = PPG_int_D_tot\n",
        "    feature.PPG_int_D_S = PPG_int_D_S\n",
        "    feature.PPG_max_D = PPG_max_D\n",
        "    \n",
        "    feature.PPG_S_High_0_10 = PPG_S_High_0_10\n",
        "    feature.PPG_S_High_0_25 = PPG_S_High_0_25\n",
        "    feature.PPG_S_High_0_33 = PPG_S_High_0_33\n",
        "    feature.PPG_S_High_0_50 = PPG_S_High_0_50\n",
        "    feature.PPG_S_High_0_66 = PPG_S_High_0_66\n",
        "    feature.PPG_S_High_0_75 = PPG_S_High_0_75\n",
        "    \n",
        "    feature.PPG_D_High_0_10 = PPG_D_High_0_10\n",
        "    feature.PPG_D_High_0_25 = PPG_D_High_0_25\n",
        "    feature.PPG_D_High_0_33 = PPG_D_High_0_33\n",
        "    feature.PPG_D_High_0_50 = PPG_D_High_0_50\n",
        "    feature.PPG_D_High_0_66 = PPG_D_High_0_66\n",
        "    feature.PPG_D_High_0_75 = PPG_D_High_0_75\n",
        "    \n",
        "    feature.D_PPG_mean = D_PPG_mean\n",
        "    feature.D_PPG_min = D_PPG_min\n",
        "    feature.D_PPG_max = D_PPG_max\n",
        "    feature.D_PPG_std = D_PPG_std\n",
        "    feature.D_PPG_amp = D_PPG_amp\n",
        "    feature.D_PPG_q_0_75 = D_PPG_0_75\n",
        "    feature.D_PPG_q_0_25 = D_PPG_0_25\n",
        "    feature.D_PPG_0_cross = D_PPG_0_cross\n",
        "    feature.D_PPG_kurt = D_PPG_kurt\n",
        "    feature.D_PPG_skew = D_PPG_skew\n",
        "    \n",
        "    feature.sex = features_anthropo['feat_sex'].values\n",
        "    feature.age = features_anthropo['feat_age'].values\n",
        "    feature.weight = features_anthropo['feat_weight'].values\n",
        "    feature.height = features_anthropo['feat_height'].values\n",
        "    feature.bmi = features_anthropo['feat_bmi'].values\n",
        "    \n",
        "    ## fin calcul features\n",
        "    \n",
        "    return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KfKGrJT7hJR"
      },
      "source": [
        "# WAVELET TRANSFORM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcCr-oywx0ID"
      },
      "source": [
        "DWT initial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDDVrwX5wmWj"
      },
      "source": [
        "import numpy as np\n",
        "import statistics\n",
        "import pywt\n",
        "from scipy.stats import kurtosis\n",
        "\n",
        "def comp_moment(feature):\n",
        "    step = int(len(feature)/2)\n",
        "    avg_temp = np.zeros([2])\n",
        "    stn_dev_temp = np.zeros([2])\n",
        "    kurto_temp = np.zeros([2])\n",
        "    for i in range(int(len(feature)/step)):\n",
        "        avg_temp[i] = np.mean(feature[step*i:step*(i+1)])\n",
        "        stn_dev_temp[i] = statistics.stdev(feature[step*i:step*(i+1)])\n",
        "        kurto_temp[i] = kurtosis(feature[step*i:step*(i+1)])\n",
        "        return (avg_temp, stn_dev_temp, kurto_temp)\n",
        "\n",
        "def dwt(y):\n",
        "    [a, d1, d2, d3, d4] = pywt.wavedec(y, 'haar', level=4)\n",
        "    \n",
        "    #Approximation coeficient\n",
        "    avg_temp, stn_dev_temp, kurto_temp = comp_moment(a)\n",
        "    avg = avg_temp\n",
        "    stn_dev = stn_dev_temp\n",
        "    kurto = kurto_temp\n",
        "\n",
        "    # d1 coffiecient\n",
        "    avg_temp, stn_dev_temp, kurto_temp = comp_moment(d1)\n",
        "    avg = np.append(avg, avg_temp)\n",
        "    stn_dev = np.append(stn_dev, stn_dev_temp)\n",
        "    kurto = np.append(kurto, kurto_temp)\n",
        "\n",
        "    # d2 coffiecient\n",
        "    avg_temp, stn_dev_temp, kurto_temp = comp_moment(d2)\n",
        "    avg = np.append(avg, avg_temp)\n",
        "    stn_dev = np.append(stn_dev, stn_dev_temp)\n",
        "    kurto = np.append(kurto, kurto_temp)\n",
        "\n",
        "    # d3 coffiecient\n",
        "    avg_temp, stn_dev_temp, kurto_temp = comp_moment(d3)\n",
        "    avg = np.append(avg, avg_temp)\n",
        "    stn_dev = np.append(stn_dev, stn_dev_temp)\n",
        "    kurto = np.append(kurto, kurto_temp)\n",
        "\n",
        "    # d4 coffiecient\n",
        "    avg_temp, stn_dev_temp, kurto_temp = comp_moment(d4)\n",
        "    avg = np.append(avg, avg_temp)\n",
        "    stn_dev = np.append(stn_dev, stn_dev_temp)\n",
        "    kurto = np.append(kurto, kurto_temp)\n",
        "\n",
        "    feature_dwt = np.append(np.append(avg, stn_dev), kurto)\n",
        "\n",
        "    return feature_dwt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0KJWozryBP9"
      },
      "source": [
        "Libraries Initial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFYThQgox7cJ"
      },
      "source": [
        "import pywt\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from scipy.signal import find_peaks, resample\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#variable global\n",
        "sample_rate = 500\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "\"\"\"\n",
        "Fungsi Lain\n",
        "\"\"\"\n",
        "def plot_color_text(filtered,predicted_beats,start_stop):\n",
        "    minimum = min(filtered)\n",
        "    maximum = max(filtered)\n",
        "    lel = [np.arange(data[0],data[1]) for data in start_stop]\n",
        "    #plt.plot(ecg_raw)\n",
        "    plt.plot(filtered)\n",
        "    for i in range(len(predicted_beats)):\n",
        "        if (predicted_beats[i] != \"N\"):\n",
        "            plt.fill_between(lel[i],minimum,maximum,facecolor='red', alpha=0.5)\n",
        "            plt.text(start_stop[i][0],maximum,predicted_beats[i])\n",
        "#    plt.scatter(peaks, [filtered[peaks[i]] for i in range(len(peaks))],c='red')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_with_rpeaks(filtered,r_peaks):\n",
        "    peaks = [filtered[peak] for peak in r_peaks]\n",
        "    plt.plot(filtered)\n",
        "    plt.scatter(r_peaks,peaks,c='red')\n",
        "#    plt.savefig(\"sinyal_clean_peaks\",dpi=1000)\n",
        "    plt.show()\n",
        "\"\"\"\n",
        "END FUNGSI LAIN\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "PREPROCESSING\n",
        "\"\"\"\n",
        "def find_signal_peaks(arraynya, minimum=0, maximum=None, freq=500):\n",
        "    dist = freq/2\n",
        "    r_peaks = find_peaks(arraynya,distance=dist,prominence=(minimum,maximum))\n",
        "    return r_peaks[0].tolist()\n",
        "\n",
        "def get_rr(r_peaks, to_sec=False,sample_rate=125):\n",
        "    rr_list = []\n",
        "    start_stop = []\n",
        "    for i in range(len(r_peaks)-2):\n",
        "        rr_list.append(r_peaks[i+1]-r_peaks[i])\n",
        "        start_stop.append([r_peaks[i],r_peaks[i+1]])\n",
        "    if (to_sec):\n",
        "        rr_list = np.divide(rr_list,sample_rate)\n",
        "    return rr_list, start_stop\n",
        "\n",
        "def moving_average(a, n=3) :\n",
        "    ret = np.cumsum(a, dtype=float)\n",
        "    ret[n:] = ret[n:] - ret[:-n]\n",
        "    return ret[n - 1:] / n\n",
        "\n",
        "def baseline_remove_ppg(signal,frequency=500):\n",
        "    half_freq = int(frequency/2)\n",
        "    baseline = resample(moving_average(signal,n=half_freq),len(signal))\n",
        "    return np.subtract(signal,baseline)\n",
        "\n",
        "def remove_signal(signal):\n",
        "    return np.zeros_like(signal)\n",
        "\n",
        "def remove_noise(noisy_signal):\n",
        "    denoised = noisy_signal\n",
        "    for i in range(5):\n",
        "        denoised = pywt.dwt(denoised,'sym8')[0]\n",
        "    for i in range(5):\n",
        "        denoised = pywt.idwt(denoised,remove_signal(denoised),wavelet='sym8')\n",
        "    return denoised\n",
        "\n",
        "def annotation_to_ppg_signal_split(signal_path, annotation_path, number):\n",
        "    signal_file = \"%s/%s.csv\" % (signal_path, number)\n",
        "    anot_file = \"%s/%s.csv\" % (annotation_path, number)\n",
        "    signal = pd.read_csv(signal_file, low_memory=False).replace(\"-\",100)\n",
        "    annotation = pd.read_csv(anot_file, low_memory=False)\n",
        "    ppg = signal.iloc[1:,2].values\n",
        "    ppg_normal, ppg_pac, ppg_pvc = [], [], []\n",
        "    start, stop, signal_class = annotation[\"start_index\"].values, annotation[\"stop_index\"].values, annotation[\"signal_class\"].values.tolist()\n",
        "    for i in range(len(annotation)):\n",
        "        if (signal_class[i] == \"N\"):\n",
        "            ppg_normal.append(ppg[start[i]:stop[i]].astype(\"float64\"))\n",
        "        elif (signal_class[i] == \"A\"):\n",
        "            ppg_pac.append(ppg[start[i]:stop[i]].astype(\"float64\"))\n",
        "        elif (signal_class[i] == \"V\"):\n",
        "            ppg_pvc.append(ppg[start[i]:stop[i]].astype(\"float64\"))\n",
        "    return ppg_normal, ppg_pac, ppg_pvc\n",
        "\n",
        "def annotation_to_ppg_signal_labeled(signal_path, annotation_path, number):\n",
        "    signal_file = \"%s/%s.csv\" % (signal_path, number)\n",
        "    anot_file = \"%s/%s.csv\" % (annotation_path, number)\n",
        "    signal = pd.read_csv(signal_file, low_memory=False).replace(\"-\",100)\n",
        "    annotation = pd.read_csv(anot_file, low_memory=False)\n",
        "    ppg = signal.iloc[1:,2].values\n",
        "    ppg_signal = []\n",
        "    start, stop, signal_class = annotation[\"start_index\"].values, annotation[\"stop_index\"].values, annotation[\"signal_class\"].values.tolist()\n",
        "    for i in range(len(annotation)):\n",
        "            ppg_cut = ppg[start[i]:stop[i]].astype(\"float64\").tolist()\n",
        "            ppg_signal.append(ppg_cut)\n",
        "    return ppg_signal, signal_class\n",
        "\n",
        "\n",
        "def read_feature_from_csv(file_name):\n",
        "    feature = pd.read_csv(file_name,low_memory=False).values\n",
        "    pp_list = feature[:,0:-1]\n",
        "    class_list = feature[:,-1]\n",
        "    return pp_list, class_list\n",
        "\n",
        "def read_feature_from_csv1(file_name):\n",
        "    feature = pd.read_csv(file_name,low_memory=False)\n",
        "    pp_list = feature.iloc[:,0:-1]\n",
        "    class_list = feature.iloc[:,-1]\n",
        "    return pp_list, class_list\n",
        "    \n",
        "def preprocess_ppg_signal(input_signal,sample_rate=500):\n",
        "    baseline_removed = baseline_remove_ppg(input_signal,frequency=sample_rate)\n",
        "    clean_ppg = remove_noise(baseline_removed)\n",
        "    return clean_ppg\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "\"\"\"\n",
        "END PREPROCESSING\n",
        "\"\"\"    \n",
        "\n",
        "\"\"\"\n",
        "FITUR EKSTRAKSI\n",
        "\"\"\"\n",
        "\n",
        "#def feature_extraction_ez(signal_array):\n",
        "#    features = []\n",
        "#    for signal in signal_array:\n",
        "#        min_signal, max_signal, mean_signal, std_signal = np.min(signal), np.max(signal), np.mean(signal), np.std(signal)\n",
        "#        feature = [min_signal, max_signal, mean_signal, std_signal]\n",
        "#        features.append(feature)\n",
        "#    return features\n",
        "#\n",
        "#def feature_extraction_medium(signal_array,window_count=4,sample_rate=500):\n",
        "#    features = []\n",
        "#    for signal in signal_array:\n",
        "#        min_signal, max_signal, mean_signal, std_signal = np.min(signal), np.max(signal), np.mean(signal), np.std(signal)\n",
        "#        peaks = find_signal_peaks(signal,minimum=max_signal*0.4)\n",
        "#        rr_list, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "#        middle = int(len(rr_list)/2)\n",
        "#        \n",
        "#        if (len(rr_list[middle:middle+window_count]) < window_count):\n",
        "#            rr_list = rr_list[middle-1:middle+window_count-1]\n",
        "#        else:\n",
        "#            rr_list = rr_list[middle:middle+window_count]\n",
        "#        \n",
        "#        feature = [min_signal, max_signal, mean_signal, std_signal]\n",
        "#        feature = np.concatenate([feature,rr_list])\n",
        "#        features.append(feature)\n",
        "#    return features\n",
        "#\n",
        "\n",
        "#QT INTERVAL FITUR\n",
        "def secondDerivative(signal):\n",
        "    new = np.zeros_like(signal)\n",
        "    for i in range(1,len(signal)-1):\n",
        "        new[i] = (signal[i+1] - (2 * signal[i]) + signal[i-1]) / (len(signal)**2)\n",
        "    return new   \n",
        "\n",
        "def detect_qt(data_peak ,to_sec=True, sample_rate=500):\n",
        "    qt_interval = []\n",
        "    start_stop = []\n",
        "    for i in range(1,len(data_peak)-1):\n",
        "        peak_sebelum = data_peak[i-1]\n",
        "        peak = data_peak[i]\n",
        "        peak_sesudah = data_peak[i+1]\n",
        "        \n",
        "        \n",
        "        deteksi_15persen  = (peak-peak_sebelum)*0.15\n",
        "#        deteksi_15persen  = peak*0.15\n",
        "        peak_q = round(peak - deteksi_15persen)\n",
        "        \n",
        "        deteksi_40persen = (peak_sesudah-peak)*0.40\n",
        "#        deteksi_40persen = peak*0.40\n",
        "\n",
        "        peak_t = round(peak + deteksi_40persen)\n",
        "        qt_interval.append(peak_t-peak_q)\n",
        "        start_stop.append([peak_q,peak_t])\n",
        "    if(to_sec):\n",
        "        qt_interval = np.divide(qt_interval,sample_rate)\n",
        "    return qt_interval , start_stop\n",
        "\n",
        "\n",
        "\n",
        "def feature_extraction_qt(signal_array,window_count=6,sample_rate=500, preprocess=True):\n",
        "    features = []\n",
        "    for signal in signal_array:\n",
        "        if (preprocess):\n",
        "            second = secondDerivative(signal)\n",
        "            signal = preprocess_ppg_signal(second)\n",
        "            signal *= 10**10\n",
        "            \n",
        "        peaks = find_signal_peaks(signal,minimum=0.2)\n",
        "        qt_list, qt_startstop = detect_qt(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "\n",
        "        middle = int(len(qt_list)/2)\n",
        "        kiri = middle-(window_count//2)\n",
        "        qt_list = qt_list[kiri:kiri+window_count].tolist()\n",
        "#        print(len(rr_list))\n",
        "        if (len(qt_list) == window_count):\n",
        "            features.append(qt_list)\n",
        "    return features\n",
        "\n",
        "import math\n",
        "#time domain features\n",
        "def feature_extraction_time_domain_features(signal_array,sample_rate=500 , preprocess=False):\n",
        "    features = []\n",
        "    i = 0\n",
        "    for signal in signal_array:\n",
        "        if (preprocess):\n",
        "            signal = preprocess_ppg_signal(signal,sample_rate=sample_rate)\n",
        "            \n",
        "        peaks = find_signal_peaks(signal,minimum=0.2)\n",
        "        rr_list, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "        \n",
        "        rata_ppi = np.mean(rr_list)\n",
        "        std_ppi = np.std(rr_list)\n",
        "        rata_sinyal = np.mean(signal)\n",
        "        std_sinyal = np.std(signal)\n",
        "        \n",
        "        if(math.isnan(rata_ppi) or math.isnan(std_ppi) or math.isnan(rata_sinyal) or math.isnan(std_sinyal)):\n",
        "          print(\"TRUE\")\n",
        "          print(i)\n",
        "          rata_ppi = np.nansum(np.mean(rr_list))\n",
        "          std_ppi = np.nansum(np.std(rr_list))\n",
        "          rata_sinyal = np.nansum(np.mean(signal))\n",
        "          std_sinyal = np.nansum(np.std(signal))\n",
        "          features.append([rata_ppi,std_ppi,rata_sinyal,std_sinyal])\n",
        "        else:\n",
        "            features.append([rata_ppi,std_ppi,rata_sinyal,std_sinyal])\n",
        "        i = i+1\n",
        "    return features\n",
        "\n",
        "#QT interval dengan sinyal second derivative\n",
        "    \n",
        "    \n",
        "#SLIDING WINDOW\n",
        "def feature_extraction_pp(signal_array,window_count=6,sample_rate=500, preprocess=False):\n",
        "    features = []\n",
        "    for signal in signal_array:\n",
        "        if (preprocess):\n",
        "            signal = preprocess_ppg_signal(signal,sample_rate=sample_rate)\n",
        "            \n",
        "        peaks = find_signal_peaks(signal,minimum=0.2)\n",
        "        rr_list, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "        \n",
        "        middle = int(len(rr_list)/2)\n",
        "        kiri = middle-(window_count//2)\n",
        "        rr_list = rr_list[kiri:kiri+window_count].tolist()\n",
        "#        print(len(rr_list))\n",
        "        if (len(rr_list) == window_count):\n",
        "            features.append(rr_list)\n",
        "    return features\n",
        "#END SLIDING WINDOW\n",
        "    \n",
        "#SKENARIO 2\n",
        "#TIME DOMAIN + SLIDING WINDOW\n",
        "def feature_extraction_time_domain_features_and_sliding_window(signal_array,window_count=6,sample_rate=500 , preprocess=False):\n",
        "    features = []\n",
        "    for signal in signal_array:\n",
        "        if (preprocess):\n",
        "            signal = preprocess_ppg_signal(signal,sample_rate=sample_rate)\n",
        "            \n",
        "        peaks = find_signal_peaks(signal,minimum=0.2)\n",
        "        rr_list, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "        \n",
        "#        TIME DOMAIN\n",
        "        rata_ppi = np.mean(rr_list)\n",
        "        std_ppi = np.std(rr_list)\n",
        "        rata_sinyal = np.mean(signal)\n",
        "        std_sinyal = np.std(signal)\n",
        "        \n",
        "#        SLIDING WINDOW\n",
        "        middle = int(len(rr_list)/2)\n",
        "        kiri = middle-(window_count//2)\n",
        "        sliding_window = rr_list[kiri:kiri+window_count].astype(\"float64\")\n",
        "        \n",
        "        if(math.isnan(rata_ppi) or math.isnan(std_ppi) or math.isnan(rata_sinyal) or math.isnan(std_sinyal)):\n",
        "          print(\"TRUE\")\n",
        "          \n",
        "          rata_ppi = np.nansum(np.mean(rr_list))\n",
        "          std_ppi = np.nansum(np.std(rr_list))\n",
        "          rata_sinyal = np.nansum(np.mean(signal))\n",
        "          std_sinyal = np.nansum(np.std(signal))\n",
        "          \n",
        "          if (len(sliding_window) == window_count):\n",
        "              array_1 = [rata_ppi,std_ppi,rata_sinyal,std_sinyal]\n",
        "              array_1.extend(sliding_window)\n",
        "              features.append(array_1)\n",
        "\n",
        "#          features.append([rata_ppi,std_ppi,rata_sinyal,std_sinyal]+sliding_window)\n",
        "        else:\n",
        "            if (len(sliding_window) == window_count):\n",
        "                  array_1 = [rata_ppi,std_ppi,rata_sinyal,std_sinyal]\n",
        "                  array_1.extend(sliding_window)\n",
        "                  features.append(array_1)    \n",
        "\n",
        "#                features.append([rata_ppi,std_ppi,rata_sinyal,std_sinyal]+sliding_window)\n",
        "\n",
        "    return features\n",
        "#END TIME DOMAIN + SLIDING WINDOW\n",
        "    \n",
        "#TIME DOMAIN + QT\n",
        "def feature_extraction_time_domain_features_and_qt(signal_array,window_count=6,sample_rate=500 , preprocess=True):\n",
        "    features = []\n",
        "    for signal in signal_array:\n",
        "        if (preprocess):\n",
        "            second = secondDerivative(signal)\n",
        "            signal = preprocess_ppg_signal(second,sample_rate=sample_rate)\n",
        "            signal *= 10**10\n",
        "            \n",
        "        peaks = find_signal_peaks(signal,minimum=0.2)\n",
        "        qt_list, qt_startstop = detect_qt(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "        \n",
        "#        TIME DOMAIN\n",
        "        rata_qt = np.mean(qt_list)\n",
        "        std_qt = np.std(qt_list)\n",
        "        rata_sinyal = np.mean(signal)\n",
        "        std_sinyal = np.std(signal)\n",
        "        \n",
        "#        QT\n",
        "        middle = int(len(qt_list)/2)\n",
        "        kiri = middle-(window_count//2)\n",
        "        qt_list = qt_list[kiri:kiri+window_count].astype(\"float64\")\n",
        "        \n",
        "        if(math.isnan(rata_qt) or math.isnan(std_qt) or math.isnan(rata_sinyal) or math.isnan(std_sinyal)):\n",
        "          print(\"TRUE\")\n",
        "          \n",
        "          rata_qt = np.nansum(np.mean(qt_list))\n",
        "          std_qt = np.nansum(np.std(qt_list))\n",
        "          rata_sinyal = np.nansum(np.mean(signal))\n",
        "          std_sinyal = np.nansum(np.std(signal))\n",
        "          if (len(qt_list) == window_count):\n",
        "              array_1 = [rata_qt,std_qt,rata_sinyal,std_sinyal]\n",
        "              array_1.extend(qt_list)\n",
        "              features.append(array_1)\n",
        "          \n",
        "#          features.append([rata_qt,std_qt,rata_sinyal,std_sinyal]+qt_list)\n",
        "          \n",
        "        else:\n",
        "            if (len(qt_list) == window_count):\n",
        "                array_1 = [rata_qt,std_qt,rata_sinyal,std_sinyal]\n",
        "                array_1.extend(qt_list)\n",
        "                features.append(array_1)\n",
        "    return features\n",
        "#END TIME DOMAIN + QT\n",
        "#END SKENARIO 2\n",
        "    \n",
        "\n",
        "#SKENARIO 3\n",
        "#TIME DOMAIN + SLIDING WINDOW + QT INTERVAL\n",
        "def feature_extraction_time_domain_features_and_sliding_window_and_qt(signal_array,window_count=6,sample_rate=500 , preprocess=True):\n",
        "    features = []\n",
        "    for signal in signal_array:\n",
        "        if (preprocess):\n",
        "            second = secondDerivative(signal)\n",
        "            signal = preprocess_ppg_signal(second,sample_rate=sample_rate)\n",
        "            signal *= 10**10\n",
        "            \n",
        "        peaks = find_signal_peaks(signal,minimum=0.2)\n",
        "        rr_list, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "        qt_list, qt_startstop = detect_qt(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "\n",
        "        \n",
        "#        SLIDING WINDOW\n",
        "        middle = int(len(rr_list)/2)\n",
        "        kiri = middle-(window_count//2)\n",
        "        sliding_window = rr_list[kiri:kiri+window_count].astype(\"float64\")\n",
        "        \n",
        "#QT            \n",
        "        middle = int(len(qt_list)/2)\n",
        "        kiri = middle-(window_count//2)\n",
        "        qt_list = qt_list[kiri:kiri+window_count].astype(\"float64\")\n",
        "\n",
        "#        TIME DOMAIN\n",
        "        rata_ppi = np.mean(sliding_window)\n",
        "        std_ppi = np.std(sliding_window)\n",
        "        rata_qt = np.mean(qt_list)\n",
        "        std_qt = np.std(qt_list)\n",
        "        rata_sinyal = np.mean(signal)\n",
        "        std_sinyal = np.std(signal)\n",
        "        \n",
        "        if(math.isnan(rata_ppi) or math.isnan(std_ppi) or math.isnan(rata_qt) or math.isnan(std_qt) or math.isnan(rata_sinyal) or math.isnan(std_sinyal)):\n",
        "          print(\"TRUE\")\n",
        "          rata_ppi = np.nansum(np.mean(sliding_window))\n",
        "          std_ppi = np.nansum(np.std(sliding_window))\n",
        "          rata_qt = np.nansum(np.mean(qt_list))\n",
        "          std_qt = np.nansum(np.std(qt_list))\n",
        "          rata_sinyal = np.nansum(np.mean(signal))\n",
        "          std_sinyal = np.nansum(np.std(signal))\n",
        "          if (len(sliding_window) == window_count and len(qt_list) == window_count):\n",
        "              array_1 = [rata_ppi,std_ppi,rata_qt,std_qt,rata_sinyal,std_sinyal]\n",
        "              array_1.extend(sliding_window)\n",
        "              array_1.extend(qt_list)\n",
        "              features.append(array_1)\n",
        "#          features.append([rata_ppi,std_ppi,rata_qt,std_qt,rata_sinyal,std_sinyal]+sliding_window+qt_list)\n",
        "          \n",
        "        else:\n",
        "            if (len(sliding_window) == window_count and len(qt_list) == window_count):\n",
        "                array_1 = [rata_ppi,std_ppi,rata_qt,std_qt,rata_sinyal,std_sinyal]\n",
        "                array_1.extend(sliding_window)\n",
        "                array_1.extend(qt_list)\n",
        "                features.append(array_1)\n",
        "                \n",
        "#                features.append([rata_ppi,std_ppi,rata_qt,std_qt,rata_sinyal,std_sinyal]+sliding_window+qt_list)\n",
        "    return features\n",
        "#END TIME DOMAIN + SLIDING WINDOW + QT INTERVAL\n",
        "\n",
        "\n",
        "def save_feature_to_csv(file_path,number, ppg_feature):\n",
        "    df_feature = pd.DataFrame(ppg_feature)\n",
        "#    df_class = pd.DataFrame(ppg_class)\n",
        "    df_feature.to_csv(\"%s/%s.csv\" % (file_path, number),index=False,header=False)\n",
        "#    df_class.to_csv(\"%s/%sclass.csv\" % (file_path, number) ,index=False,header=False)\n",
        "#def feature_extraction_pp(signal_array,window_count=4,sample_rate=500, preprocess=False):\n",
        "#    features = []\n",
        "#    for signal in signal_array:\n",
        "#        if (preprocess):\n",
        "#            signal = preprocess_ppg_signal(signal,sample_rate=sample_rate)\n",
        "#            \n",
        "#        peaks = find_signal_peaks(signal,minimum=0.2)\n",
        "#        rr_list, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=sample_rate)\n",
        "##        middle = int(len(rr_list)/2)\n",
        "#        \n",
        "##        if (len(rr_list[middle:middle+window_count]) < window_count):\n",
        "##            rr_list = rr_list[middle-1:middle+window_count-1]\n",
        "##        else:\n",
        "##            rr_list = rr_list[middle:middle+window_count]\n",
        "##        \n",
        "##        print(rr_list)\n",
        "##        feature = np.concatenate([feature,rr_list])\n",
        "#        features.append(rr_list)\n",
        "#    return features\n",
        "#    \n",
        "\"\"\"\n",
        "END FITUR EKSTRAKSI\n",
        "\"\"\"\n",
        "\n",
        "def class_count(r_class):\n",
        "#    count = BeatCount()\n",
        "    kelas = [0,0,0]\n",
        "    for cl in r_class:\n",
        "        if (cl == \"N\"):\n",
        "            kelas[0] += 1\n",
        "        elif (cl == \"A\"):\n",
        "            kelas[1] += 1\n",
        "        elif (cl == \"V\"):\n",
        "            kelas[2] += 1\n",
        "    return kelas\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Metrics\n",
        "\"\"\"\n",
        "def generate_original_and_predicted_class(original_class,predicted_class):\n",
        "    temp = [0,0,0,0,0,0]\n",
        "    for i in range(len(original_class)):\n",
        "        if (original_class[i] == \"N\"):\n",
        "            temp[0] += 1\n",
        "        elif (original_class[i] == \"A\"):\n",
        "            temp[1] += 1\n",
        "        elif (original_class[i] == \"V\"):\n",
        "            temp[2] += 1\n",
        "        if (predicted_class[i] == \"N\"):\n",
        "            temp[3] += 1\n",
        "        elif (predicted_class[i] == \"A\"):\n",
        "            temp[4] += 1\n",
        "        elif (predicted_class[i] == \"V\"):\n",
        "            temp[5] += 1\n",
        "    return temp\n",
        "\n",
        "def generate_confusion_matrix(number,original_class,predicted_class,smoothing=False,smoothing_value=10^-4):\n",
        "    count_original = class_count(original_class)\n",
        "    count_predicted = class_count(predicted_class)\n",
        "    tp_PAC, tn_PAC, fp_PAC, fn_PAC = 0,0,0,0\n",
        "    tp_PVC,tn_PVC,fp_PVC,fn_PVC = 0,0,0,0\n",
        "    acc_PAC, acc_PVC = 0,0\n",
        "    sp_PAC, sp_PVC = 0,0\n",
        "    sn_PAC, sn_PVC = 0,0\n",
        "    f1_PAC, f1_PVC = 0,0\n",
        "    minimum_number = smoothing_value\n",
        "    for i in range(len(original_class)):\n",
        "        ori = original_class[i]\n",
        "        tes = predicted_class[i]\n",
        "        if (ori == \"N\"):\n",
        "            if (tes == \"V\"):\n",
        "                fp_PVC += 1\n",
        "                tn_PAC += 1\n",
        "            elif (tes == \"A\"):\n",
        "                fp_PAC += 1\n",
        "                tn_PVC += 1\n",
        "            else:\n",
        "                tn_PVC += 1\n",
        "                tn_PAC += 1\n",
        "        elif (ori == \"V\"):\n",
        "            if (tes == \"V\"):\n",
        "                tp_PVC += 1\n",
        "                tn_PAC += 1\n",
        "            elif (tes == \"A\"):\n",
        "                fp_PAC += 1\n",
        "                fn_PVC += 1\n",
        "            else:\n",
        "                tn_PAC += 1\n",
        "                fn_PVC += 1\n",
        "        elif (ori == \"A\"):\n",
        "            if (tes == \"V\"):\n",
        "                fp_PVC += 1\n",
        "                fn_PAC += 1\n",
        "            elif (tes == \"A\"):\n",
        "                tp_PAC += 1\n",
        "                tn_PVC += 1\n",
        "            else:\n",
        "                tn_PVC += 1\n",
        "                fn_PAC += 1\n",
        "    if (smoothing):\n",
        "        acc_PAC = (tn_PAC+tp_PAC+minimum_number) / (tn_PAC+tp_PAC+fn_PAC+fp_PAC+minimum_number) * 100\n",
        "        acc_PVC = (tn_PVC+tp_PVC+minimum_number) / (tn_PVC+tp_PVC+fn_PVC+fp_PVC+minimum_number) * 100\n",
        "        sp_PAC = (tn_PAC+minimum_number)/(tn_PAC+fp_PAC+minimum_number) * 100\n",
        "        sp_PVC = (tn_PVC+minimum_number)/(tn_PVC+fp_PVC+minimum_number) * 100\n",
        "        sn_PAC = (tp_PAC+minimum_number) / (tp_PAC+fn_PAC+minimum_number) * 100\n",
        "        sn_PVC = (tp_PVC+minimum_number) / (tp_PVC+fn_PVC+minimum_number) * 100\n",
        "    else:\n",
        "        try:\n",
        "            acc_PAC = (tn_PAC+tp_PAC) / (tn_PAC+tp_PAC+fn_PAC+fp_PAC) * 100\n",
        "        except:\n",
        "            acc_PAC = 0\n",
        "        try:\n",
        "            acc_PVC = (tn_PVC+tp_PVC) / (tn_PVC+tp_PVC+fn_PVC+fp_PVC) * 100\n",
        "        except:\n",
        "            acc_PVC = 0\n",
        "        try: \n",
        "            sp_PAC = tn_PAC/(tn_PAC+fp_PAC) * 100\n",
        "        except:\n",
        "            sp_PAC = 0\n",
        "        try:\n",
        "            sp_PVC = tn_PVC/(tn_PVC+fp_PVC) * 100\n",
        "        except:\n",
        "            sp_PVC = 0\n",
        "        try:\n",
        "            sn_PAC = (tp_PAC) / (tp_PAC+fn_PAC) * 100\n",
        "        except:\n",
        "            sn_PAC = 0\n",
        "        try:\n",
        "            sn_PVC = (tp_PVC) / (tp_PVC+fn_PVC) * 100\n",
        "        except:\n",
        "            sn_PVC = 0\n",
        "    temp = [number,count_original[0], count_original[1], count_original[2], count_predicted[0], count_predicted[1], count_predicted[2], tp_PAC, fp_PAC, tn_PAC, fn_PAC,round(acc_PAC,2), round(sp_PAC,2),sn_PAC, tp_PVC, fp_PVC, tn_PVC, fn_PVC,round(acc_PVC,2),round(sp_PVC,2),sn_PVC]\n",
        "    columns = [\"Number\", \"Normal\",\"PAC\",\"PVC\",\"pred_Normal\", \"pred_PAC\",\"pred_PVC\",\"tp_PAC\",\"fp_PAC\",\"tn_PAC\",\"fn_PAC\",\"acc_PAC\",\"sp_PAC\",\"sn_PAC\",\"tp_PVC\", \"fp_PVC\", \"tn_PVC\", \"fn_PVC\",\"acc_PVC\",\"sp_PVC\",\"sn_PVC\"]\n",
        "#    print(len(temp), len(columns))\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhvVSQ5o_sEo"
      },
      "source": [
        "# DWT EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgmxEP8eDj3K"
      },
      "source": [
        "import numpy as np\n",
        "import csv, os, glob, ntpath\n",
        "from dwt import dwt\n",
        "import pandas as pd\n",
        "from libraries import *\n",
        "\n",
        "x = np.array([])\n",
        "y = np.array([])\n",
        "path = 'D:\\\\TA ORANG\\\\Ryan\\\\test\\\\data1'\n",
        "pathname = \"D:\\\\TA ORANG\\\\Ryan\\\\test\\\\data1\\\\\"\n",
        "#os.chdir(path)\n",
        "#files = glob.glob('*.{}'.format('csv'))\n",
        "files = glob.glob(os.path.join(path, '*.csv'))\n",
        "file_path = \"fitur/dwt/\"\n",
        "\n",
        "##Reading file and saving to variables\n",
        "for filename in files:\n",
        "    with open(filename) as csvfile:\n",
        "        print(filename)\n",
        "        f_read = csv.reader(csvfile, delimiter = ',')\n",
        "        number = filename.replace(\".csv\",\"\")\n",
        "        number = number.replace(pathname,\"\")\n",
        "        hasilfile = file_path+number+\".csv\"\n",
        "        csv_out = open(hasilfile, \"w\")  \n",
        "        print(hasilfile)\n",
        "        #ppg_signal, ppg_class = annotation_to_ppg_signal_labeled(\"data\",\"annotation\",number)\n",
        "        #print(ppg_signal)\n",
        "        for row in f_read:\n",
        "            if (row[0] != \"'sample interval'\" and row[0] != \"'0.002 sec'\") : \n",
        "                #taking x parameter only for plotting the signal\n",
        "                #x = np.append(x, [float(j) for j in row]) -- Comented as only using 1 value\n",
        "                x = np.append(x, float(row[0]))\n",
        "\n",
        "                #Cleaning signal data\n",
        "                if row[2][:1] == '-' : \n",
        "                    if row[2][1:len(row[2])] == '' :\n",
        "                        y = np.append(y, 0)\n",
        "                    else :\n",
        "                        curr = float(row[2][1:len(row[2])])\n",
        "                        curr = float(-curr)\n",
        "                        y = np.append(y, curr)\n",
        "                else :\n",
        "                    y = np.append(y, float(row[2]))\n",
        "\n",
        "        feature_dwt = dwt(y)\n",
        "        feature_dwt1=np.array(feature_dwt.flatten())\n",
        "        a_str=','.join(str(x) for x in feature_dwt1)\n",
        "        print(a_str)\n",
        "        csv_out.write(a_str)\n",
        "        #save_feature_to_csv(file_path,number,a_str)\n",
        "        #dwt_feature = pd.DataFrame(feature_dwt)\n",
        "        #a_str.to_csv(\"%s/%s.csv\" % (file_path, number),index=False,header=False)\n",
        "        #print(feature_dwt)\n",
        "        csv_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkiVsFAM1LxR"
      },
      "source": [
        "# **PPG FeatureExtraction**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLyWBfukIoZQ"
      },
      "source": [
        "#from scipy.signal import argrelmax, argrelmin\n",
        "import numpy as np \n",
        "from libraries import *\n",
        "import os\n",
        "from time import time\n",
        "\"\"\"\n",
        "Main Program\n",
        "\"\"\"\n",
        "\n",
        "#ppg_normal, ppg_pac, ppg_pvc = annotation_to_ppg_signal(\"/home/ipat/Project/TA/KODING_FINAL/data\",\"/home/ipat/Project/TA/KODING_FINAL/annotation\",\"212\")\n",
        "#all_feature = []\n",
        "\n",
        "lokasi_file = \"fitur/gabungan\"\n",
        "\n",
        "for filename in os.listdir(\"annotation\"):\n",
        "    if (\".csv\" in filename):\n",
        "        start = time()\n",
        "        nomor = filename.replace(\".csv\",\"\")\n",
        "        print(nomor)\n",
        "        try:\n",
        "            ppg_signal, ppg_class = annotation_to_ppg_signal_labeled(\"data\",\"annotation\",nomor)\n",
        "            print(ppg_signal)\n",
        "#            ganti ganti\n",
        "            ppg_feature = feature_extraction_time_domain_features_and_sliding_window_and_qt(ppg_signal,preprocess=True)\n",
        "\n",
        "            for i in range(len(ppg_feature)):\n",
        "                ppg_feature[i].append(ppg_class[i])\n",
        "            save_feature_to_csv(lokasi_file,nomor,ppg_feature)\n",
        "            print(time() - start, \"second \"+lokasi_file)\n",
        "        except Exception as e:\n",
        "            print(\"failed\",e)\n",
        "            print(time() - start, \"second \"+lokasi_file)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu02aIYKyfLZ"
      },
      "source": [
        "# *PPG LEARNING FEATURE*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhZUMvmZyefP"
      },
      "source": [
        "#from scipy.signal import argrelmax, argrelmin\n",
        "import numpy as np \n",
        "from libraries import *\n",
        "import os\n",
        "from time import time\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\"\"\"\n",
        "Generate Features and Classes\n",
        "\"\"\"\n",
        "all_feature, all_class = [], []\n",
        "\n",
        "skenario = \"fitur/\"\n",
        "nama_fitur = \"dwt\"\n",
        "\n",
        "fitur_path = skenario+nama_fitur\n",
        "model_path = \"model_skenario/\"+skenario\n",
        "model_filename = \"KNN_7_\"+nama_fitur\n",
        "\n",
        "for filename in os.listdir(fitur_path): \n",
        "    if (\".csv\" in filename):\n",
        "        start = time()\n",
        "        nomor = filename.replace(\".csv\",\"\")\n",
        "        print(nomor)\n",
        "        try:\n",
        "            feature_list, class_list = read_feature_from_csv(fitur_path+\"/%s.csv\" % nomor)\n",
        "            if (len(all_feature) == 0):\n",
        "                all_feature = feature_list\n",
        "                all_class= class_list\n",
        "            else:\n",
        "                all_feature = np.concatenate([all_feature, feature_list])\n",
        "                all_class = np.concatenate([all_class, class_list])\n",
        "#            print (class_count(class_list))\n",
        "            print(time() - start, \"second\")\n",
        "        except Exception as e:\n",
        "            print(\"failed\",e)\n",
        "            print(time() - start, \"second\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Split Train and Test\n",
        "\"\"\"            \n",
        "from imblearn.over_sampling import SMOTE\n",
        "ros = SMOTE()\n",
        "X_resampled, y_resampled = ros.fit_resample(all_feature,all_class)\n",
        "count_original = class_count(all_class)\n",
        "count_resampled = class_count(y_resampled)\n",
        "\n",
        "\"\"\"\n",
        "kFold Algorithm\n",
        "\"\"\"\n",
        "from sklearn.model_selection import KFold # import KFold\n",
        "kf = KFold(n_splits=10) # Define the split - into 2 folds \n",
        "accuracy_list = []\n",
        "for train_index, test_index in kf.split(X_resampled):\n",
        "    X_train, X_test = X_resampled[train_index], X_resampled[test_index]\n",
        "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
        "    \n",
        "    \"\"\"\n",
        "    Import and fit classifier\n",
        "    \"\"\"\n",
        "    #import skflow\n",
        "    #\n",
        "#    aktivasi = \"tanh\"\n",
        "#    sizes = 12\n",
        "#    clf = MLPClassifier(activation=aktivasi,verbose=1,hidden_layer_sizes=12,max_iter=300)\n",
        "    #clf = LogisticRegression()\n",
        "    clf = KNeighborsClassifier(n_neighbors=7)\n",
        "    \n",
        "    clf.fit(X_train,y_train)\n",
        "    pred = clf.predict(X_test)\n",
        "    \"\"\"\n",
        "    Training Metrics\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_test,pred)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "\n",
        "average_accuracy = np.average(accuracy_list)\n",
        "accuracy_list.append(average_accuracy)\n",
        "\n",
        "akurasi_tocsv=pd.DataFrame(accuracy_list)\n",
        "akurasi_tocsv.to_csv(\"%s_akurasi.csv\" % (model_path+model_filename),index=False,header=False)\n",
        "print(average_accuracy)\n",
        "\"\"\"\n",
        "Save Model To File using Pickle\n",
        "\"\"\"\n",
        "import pickle\n",
        "#pickle.dump(clf,open(model_path+\"/logistic_regression.sav\",\"wb\"))\n",
        "#pickle.dump(clf,open(model_path+\"/ANN_%s_%s.sav\" % (sizes, aktivasi),\"wb\"))\n",
        "#pickle.dump(clf,open(model_path+\"/decision_tree.sav\",\"wb\"))\n",
        "#pickle.dump(clf,open(model_path+\"/decision_tree.sav\",\"wb\"))\n",
        "\n",
        "pickle.dump(clf,open(model_path+model_filename+\".sav\",\"wb\"))\n",
        "#hehe = generate_original_and_predicted_class(y_test,pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_DJ5505y1uK"
      },
      "source": [
        "# PPG PredictFromModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHlusluMyr5o"
      },
      "source": [
        "#from scipy.signal import argrelmax, argrelmin\n",
        "import numpy as np \n",
        "from libraries import *\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "\"\"\"\n",
        "Import classifier\n",
        "\"\"\"\n",
        "skenario = \"fitur_skenario_3/\"\n",
        "nama_fitur = \"gabungan\"\n",
        "\n",
        "fitur_path = skenario+nama_fitur\n",
        "lokasi_model = \"model_skenario/\"+skenario\n",
        "\n",
        "import pickle\n",
        "clf = pickle.load(open(lokasi_model+\"/KNN_7_\"+nama_fitur+\".sav\",\"rb\"))\n",
        "\"\"\"\n",
        "Generate Features and Classes\n",
        "\"\"\"\n",
        "hasil = []\n",
        "for filename in os.listdir(fitur_path):\n",
        "    if (\".csv\" in filename):\n",
        "        start = time()\n",
        "        nomor = filename.replace(\".csv\",\"\")\n",
        "        print(nomor)\n",
        "        try:\n",
        "            feature_list, class_list = read_feature_from_csv(fitur_path+\"/%s.csv\" % nomor)\n",
        "            pred = clf.predict(feature_list)\n",
        "            hasil.append(generate_confusion_matrix(nomor,class_list,pred,smoothing=False))\n",
        "        except Exception as e:\n",
        "            print(\"failed\",e)\n",
        "            print(time() - start, \"second\")\n",
        "\n",
        "\"\"\"\n",
        "Export Result to CSV\n",
        "\"\"\"\n",
        "columns_list = [\"Sample_Data\", \"Normal\",\"PAC\",\"PVC\",\"pred_Normal\", \"pred_PAC\",\"pred_PVC\",\"True_Positive_PAC\",\"False_Positive_PAC\",\"True_Negative_PAC\",\"False_Negative_PAC\",\"Accuracy_PAC\",\"Spesifisity_PAC\",\"sn_PAC\",\"True_Positive_PVC\", \"False_Positive_PVC\", \"True_Negative_PVC\", \"False_Negative_PVC\",\"Accuracy_PVC\",\"Spesifisity_PVC\",\"sn_PVC\"]\n",
        "pd.DataFrame(hasil,columns=columns_list).to_csv(\"hasil_skenario/\"+fitur_path+\"/confusion_matrix_fitur_\"+nama_fitur+\".csv\",index=False)\n",
        "\n",
        "data = pd.DataFrame(hasil)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}